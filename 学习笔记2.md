#HADOOP
***
## 简介 ##
Hadoop是一个大数据应用平台，可应用于大数据存储、计算、分析。

**组成**

- 分布式文件系统HDFS
- 资源分配系统 Yarn
- 分布式运算矿建MapReduce


## HDFS ##
分布式文件系统、基于流数据模式访问和处理超大文件的需求而开发的、适合应用在大规模数据集上。

*ps：实验项目由于使用本机配置限制，均在单机环境下配备的伪分布式Hadoop系统*

1. 实验一：Hadoop伪分布安装部署
	
	略
	

2. 实验二：Hadoop常用命令
	
	hadoop fs -ls <path>列出目录及文件

	hadoop fs -ls -R <path>递归列出目录及文件

	hadoop fs -put <src> <des>上传

	hadoop fs -get <des> <src>下载

	hadoop fs -rm <path>删除 -rmr级联删除
3. 实验三：HDFS I/O

	hdfs读取文件到本地这里只提到使用IOUtils.copyBytes方法

读取hdfs上单个文件内容部分代码如下：

	` 
	//初始化输入流
	FileSystem fs =null;
	FSDataInputStream in = null;

	try{
	fs = FileSystem.get(conf);
	//打开文件
	in =fs.open(new Path(uri));
	//将输入流拷贝到显示屏输出，每次拷贝40496字节，且流不自动关闭
	IOUtils.copyBytes(in,System.out,4096,false);
	} catch (IOEception e)
	.....
	finally {
	if (in != null) {
	//关闭输入流
		IOUtils.closeStream(in);
		}
	if(fs != null) {
	try {
	//关闭文件系统
	fs.close()；
	...	
	`

----------

	** hdfs上多个文件读取 （待补充） **



Mar.7 补充
----------


1. 	MapReduce 环境配置 conf中
	
	hadoop-local.xml包含hadoop默认文件系统和jobtracker的默认配置信息
	` < configuration >   <property> <name>fs.default.name</name>   <value>file :///</value>     </property>     <property>    <name>mapred.job.tracker</name >     <value>local</value>     </property>     </configuration >`
	hadoop-localhost.xml丈件中的设置指向本地主机上运行的namenode和jobtracker： 
	` < configuration >      <property>  <name>fs.default.name</name>   <value>hdfs ://localhost/</value>      </property>      <property>  <name>mapred.job.tracker</name>   <value>localhost : 8021</value>     </property>     </configuration>
`
	hadoop-clusterxml文件包含集群内namenode和jobtracker的详细信息。
	` <configuration>    <property>  <name>fs.default.name</name>   <value>hdfs ://namenode/</value>    </property>    <property>  <name>mapred.job.tracker</name>  <value>jobtracker: 8021</value>    </property>    </configuration>
`
	
	
2. 	WordCount 编程
	
	
	FileInputFormat类：读取输入文件，并将这些文件分割为一个或InputSplits;setInputFormatClass()可设置文件输入的格式。
	
	各种InputFormat：1. TextInputFormat，默认的格式，每一行是一个单独的记录，并且作为value，文件的偏移值作为key 2.KeyValueInputFormat，这个格式每一行也是一个单独的记录，但是Key和Value用Tab隔开；hadoop默认 的OutputFormat，即TextOutputFormat的输出结果就是此种类型 

	inputSplit定义单个map任务的输入数据（的大小），一个MapReduce程序被统称为一个Job，可能有很多个任务构成。

	RecordReader实际上定义了如何从数据上转化为一 个(key,value)对，从而输出到Mapper类中；TextInputFormat提供了LineRecordReader

	
> Hadoop将输入数据切分成若干个输入分片(input split)，并将 每个split交给一个Map Task处理；Map Task不断的从对应 的split中解析出一个个key/value，并调用map()函数处理，处 理完之后根据Reduce Task个数将结果分成若干个分 片(partition)写到本地磁盘；同时，每个Reduce Task从每 个Map Task上读取属于自己的那个partition，然后基于排序的 方法将key相同的数据聚集在一起，调用reduce()函数处理，并 将结果输出到文件中	


设计思路：

map: (k1; v1) → [(k2; v2)]

输入：键值对(k1; v1)表示的数据

处理：文档数据记录(如文本文件中的行，或数据表格 中的行)将以“键值对”形式传入map函数；map函数将 处理这些键值对，并以另一种键值对形式输出处理的 一组键值对中间结果[(k2; v2)]

输出：键值对[(k2; v2)]表示的一组中间数据

reduce: (k2; [v2]) → [(k3; v3)]

输入： 由map输出的一组键值对[(k2; v2)] 将被 进行合并处理将同样主键下的不同数值合并到一个 列表[v2]中，故reduce的输入为(k2; [v2]) 

处理：对传入的中间结果列表数据进行某种整理或 进一步的处理,并产生最终的某种形式的结果输 出[(k3; v3)] 。 

输出：最终输出结果[(k3; v3)]

ps:MapReduce好难...看不懂。

Mar.8 （待补充HBASE）
----------


**Q1：HAFS与HBASE区别**

**Q2：HDFS体系结构？namenode与DataNode作用？**
	
1. 文件切分成块（默认大小128M，以块为单位，每个块有多个副本存储在不同的机器上，副本数可在文件生成时指定
2. namenode（主节点）：存储元数据如文件 名，文件目录结构，文件 属性（生成时间,副本数, 文件权限），以及每个文 件的块列表以及块所在 的DataNode等等

3. DataNode：本地文件系统存储文件 块数据，以及块数据的校验

***简单的说就是namenode负责集群中与存储相关的调度，datanode负责具体的存储***

ps：datanode和namenode细节知识点还有很多，待后续补充。

----------

